# -*- coding: utf-8 -*-
"""Q2_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-smQcKd991Nt45788aeuCn2kRfnBvn-l

#### Q2) In this question, you will need to implement the boosting algorithm AdaBoost as well as a Bagging, in both cases using decision trees as the base classifiers.

Here, for every dataset, we have implemented shallow and deep decision tree adaptive boosting models.

## Spam Dataset - Adaboost
#### https://archive.ics.uci.edu/ml/datasets/spambase

importing required packages
"""

import numpy as np
import pandas as pd

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score

"""importing data"""

# data
df = pd.read_csv('spambase.data', header = None)

# columns
headerName = pd.read_csv('spambase.names', sep = ':', skiprows=range(0, 33), header = None)
col = list(headerName[0])
col.append('Target')
df.columns = col

# df.head()

#splitting data
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = "Target").values, df["Target"].values, random_state = 42)

"""functions for calculation of error, beta, and updating weights"""

def epsilonT(wTi, y, predicted_y):

  et = wTi[(predicted_y != y)].sum()

  return et

def betaT(et):
  bt = 0.5 * (np.log((1 - et) / et))

  return bt

def updateWTi(wTi, y, predicted_y, bt):
  wt2 = (wTi * (np.exp(-bt * y * predicted_y)))

  return wt2

"""### Shallow Tree

Model design:
"""

class model_ada:
    
    # constructor for model class
    def __init__(self):
        self.T = 0
        self.err_train = []
        self.err_pred = []
        self.bt_s = []
        self.ht_s = []
        
        

    def modelFit(self, X_train, y, T):
        
        self.T = T
        self.bt_s = [] 
        self.err_train = []
        
        w_i = np.ones(len(y)) * 1 / len(y) 

        for t in range(0, T):
            #TRAINING WITH WT
            # Using decision tree classifier
            ht = DecisionTreeClassifier(max_depth = 1)
            ht.fit(X_train, y, sample_weight = w_i)
            y_pred = ht.predict(X_train)
            
            self.ht_s.append(ht)

            # weighted training error calculation
            et = epsilonT(w_i, y, y_pred)
            self.err_train.append(et)

            # choose beta
            bt = betaT(et)
            self.bt_s.append(bt)

            #update all instance weights
            w_i = updateWTi(w_i, y, y_pred, bt)



    def modelPredict(self, X_test):
            # data with predictions[weak_data_predictions]
            weak = pd.DataFrame(index = range(len(X_test)), columns = range(self.T)) 

            # prediction
            for t in range(self.T):
                predict_yT = self.ht_s[t].predict(X_test) * self.bt_s[t]
                weak.iloc[:,t] = predict_yT

            y_pred = (1 * np.sign(weak.T.sum())).astype(int)

            return y_pred

# fitting
ab = model_ada()
ab.modelFit(X_train, y_train, T = 400)

# prediction
predictions = ab.modelPredict(X_test)

print("Classification report: \n", classification_report(y_test, predictions))

print("Score:", roc_auc_score(y_test, predictions))

"""For shallow decision tree Adaboost model, the accuracy is 0.96.

### Deep Tree
"""

class model_ada:
    
    # constructor for model class
    def __init__(self):
        self.T = 0
        self.err_train = []
        self.err_pred = []
        self.bt_s = []
        self.ht_s = []
        
        

    def modelFit(self, X_train, y, T):
        
        self.T = T
        self.bt_s = [] 
        self.err_train = []
        
        w_i = np.ones(len(y)) * 1 / len(y) 

        for t in range(0, T):
            #TRAINING WITH WT
            # Using decision tree classifier
            ht = DecisionTreeClassifier(max_depth = 100)
            ht.fit(X_train, y, sample_weight = w_i)
            y_pred = ht.predict(X_train)
            
            self.ht_s.append(ht)

            # weighted training error calculation
            et = epsilonT(w_i, y, y_pred)
            self.err_train.append(et)

            # choose beta
            bt = betaT(et)
            self.bt_s.append(bt)

            #update all instance weights
            w_i = updateWTi(w_i, y, y_pred, bt)



    def modelPredict(self, X_test):
            # data with predictions[weak_data_predictions]
            weak = pd.DataFrame(index = range(len(X_test)), columns = range(self.T)) 

            # prediction
            for t in range(self.T):
                predict_yT = self.ht_s[t].predict(X_test) * self.bt_s[t]
                weak.iloc[:,t] = predict_yT

            y_pred = (1 * np.sign(weak.T.sum())).astype(int)

            return y_pred

# fitting
ab = model_ada()
ab.modelFit(X_train, y_train, T = 400)

# prediction
predictions = ab.modelPredict(X_test)

print("Classification report: \n", classification_report(y_test, predictions))

print("Score:", roc_auc_score(y_test, predictions))

"""For deep decision tree Adaboost model, the accuracy is 0.95."""



"""

## Spam Dataset - Bagging"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.pipeline import Pipeline

from sklearn.metrics import accuracy_score
import seaborn as sns

"""#### Training and testing bagging models for shallow and deep decision trees

Here, we are using 2 models, first - shallow tree, and second: deep tree.
"""

Collection_bagging = ["model1", "model2"]

models = []

# creating 2 classifiers = first: shallow , second: deep decision tree

models.append(BaggingClassifier(DecisionTreeClassifier(max_depth = 1, random_state=42), bootstrap=True, random_state=42))
models.append(BaggingClassifier(DecisionTreeClassifier(max_depth = 100, random_state=42), bootstrap=True, random_state=42))

"""we have created a pipeline for these models."""

bagging_train_accuracies = []
bagging_test_accuracies = []

for model in models:
    bagging_pipeline = Pipeline(steps = [
              ('bagging_classifier', model)
           ])
    
    
    print(" \n For model", model)
    bagging_model = bagging_pipeline.fit(X_train, y_train)

    print("\nTraining:")
    
    bagging_train_predictions = bagging_model.predict(X_train)
    bagging_train_accuracy = accuracy_score(y_train, bagging_train_predictions)
    bagging_train_accuracies.append(bagging_train_accuracy)
    print("training accuracy of model: ", bagging_train_accuracy)
    
    print("\n Testing: ")

    bagging_test_predictions = bagging_model.predict(X_test)
    bagging_test_accuracy = accuracy_score(y_test, bagging_test_predictions)
    bagging_test_accuracies.append(bagging_test_accuracy)
    print("testing accuracy of model:", bagging_test_accuracy)

    print("\n ================================================ \n")

accuracy_table = pd.DataFrame({'model' : models, 'Training Accuracy' : bagging_train_accuracies, 'Testing Accuracy' : bagging_test_accuracies})

"""Plot for training and testing accuracies:"""

ax = sns.lineplot(data=[accuracy_table['Training Accuracy'], accuracy_table['Testing Accuracy']], dashes=False, markers=True)
ax.set_xticks(range(len(accuracy_table['model'])))
ax.set_xticklabels(accuracy_table['model'])

"""As the maximum accuracy for bagging classifier is 0.94, and for adaboost model is 0.96; the boosting method is efficient for spam data."""



"""## Credit Dataset - Ada Boost
#### https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
"""

import numpy as np
import pandas as pd

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score

# data
df = pd.read_csv('german.data', header = None, sep = '\s+')

headers = ["Existing-Account-Status","Month-Duration","Credit-History","Purpose","Credit-Amount","Saving-Acount","Present-Employment","Instalment-Rate","Sex","Guarantors","Residence","Property","Age","Installment","Housing","Existing-Credits","Job","Num-People","Telephone","Foreign-Worker","Status"]

df.to_csv("gn.csv", header=headers, index=False)

df2 = pd.read_csv("gn.csv")

df2.head()

CategoricalFeatures=['Existing-Account-Status','Credit-History','Purpose','Saving-Acount', 'Present-Employment', 'Sex','Guarantors','Property','Installment','Housing','Job','Telephone','Foreign-Worker']

data_encode=df2.copy()
data_visual=df2.copy()
data_encode.head(5)

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

for x in CategoricalFeatures:
    data_encode[x]=label_encoder.fit_transform(data_encode[x])
    data_encode[x].unique()

data_encode.head(4)

#splitting data
X_train, X_test, y_train, y_test = train_test_split(data_encode.drop(columns = "Status").values, data_encode["Status"].values, random_state = 42)

def epsilonT(wTi, y, predicted_y):

  et = wTi[(predicted_y != y)].sum()

  return et

def betaT(et):
  bt = 0.5 * (np.log((1 - et) / et))

  return bt

def updateWTi(wTi, y, predicted_y, bt):
  wt2 = (wTi * (np.exp(-bt * y * predicted_y)))

  return wt2

"""### Shallow Tree"""

class model_ada:
    
    # constructor for model class
    def __init__(self):
        self.T = 0
        self.err_train = []
        self.err_pred = []
        self.bt_s = []
        self.ht_s = []
        
        

    def modelFit(self, X_train, y, T):
        
        self.T = T
        self.bt_s = [] 
        self.err_train = []
        
        w_i = np.ones(len(y)) * 1 / len(y) 

        for t in range(0, T):
            #TRAINING WITH WT
            # Using decision tree classifier
            ht = DecisionTreeClassifier(max_depth = 1)
            ht.fit(X_train, y, sample_weight = w_i)
            y_pred = ht.predict(X_train)
            
            self.ht_s.append(ht)

            # weighted training error calculation
            et = epsilonT(w_i, y, y_pred)
            self.err_train.append(et)

            # choose beta
            bt = betaT(et)
            self.bt_s.append(bt)

            #update all instance weights
            w_i = updateWTi(w_i, y, y_pred, bt)



    def modelPredict(self, X_test):
            # data with predictions[weak_data_predictions]
            weak = pd.DataFrame(index = range(len(X_test)), columns = range(self.T)) 

            # prediction
            for t in range(self.T):
                predict_yT = self.ht_s[t].predict(X_test) * self.bt_s[t]
                weak.iloc[:,t] = predict_yT

            y_pred = (1 * np.sign(weak.T.sum())).astype(int)

            return y_pred

# fitting
ab = model_ada()
ab.modelFit(X_train, y_train, T = 400)

# prediction
predictions = ab.modelPredict(X_test)

print("Classification report: \n", accuracy_score(y_test, predictions))

"""For shallow decision tree Adaboost model, the accuracy is 0.71.

### Deep Tree
"""

class model_ada:
    
    # constructor for model class
    def __init__(self):
        self.T = 0
        self.err_train = []
        self.err_pred = []
        self.bt_s = []
        self.ht_s = []
        
        

    def modelFit(self, X_train, y, T):
        
        self.T = T
        self.bt_s = [] 
        self.err_train = []
        
        w_i = np.ones(len(y)) * 1 / len(y) 

        for t in range(0, T):
            #TRAINING WITH WT
            # Using decision tree classifier
            ht = DecisionTreeClassifier(max_depth = 100)
            ht.fit(X_train, y, sample_weight = w_i)
            y_pred = ht.predict(X_train)
            
            self.ht_s.append(ht)

            # weighted training error calculation
            et = epsilonT(w_i, y, y_pred)
            self.err_train.append(et)

            # choose beta
            bt = betaT(et)
            self.bt_s.append(bt)

            #update all instance weights
            w_i = updateWTi(w_i, y, y_pred, bt)



    def modelPredict(self, X_test):
            # data with predictions[weak_data_predictions]
            weak = pd.DataFrame(index = range(len(X_test)), columns = range(self.T)) 

            # prediction
            for t in range(self.T):
                predict_yT = self.ht_s[t].predict(X_test) * self.bt_s[t]
                weak.iloc[:,t] = predict_yT

            y_pred = (1 * np.sign(weak.T.sum())).astype(int)

            return y_pred

# fitting
ab = model_ada()
ab.modelFit(X_train, y_train, T = 400)

# prediction
predictions = ab.modelPredict(X_test)

print("Classification report: \n", accuracy_score(y_test, predictions))



"""## Credit Dataset - Bagging"""





from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.pipeline import Pipeline

from sklearn.metrics import accuracy_score
import seaborn as sns

"""#### Training and testing bagging models for shallow and deep decision trees"""

Collection_bagging = ["model1", "model2"]

models = []

# creating 2 classifiers = first: shallow , second: deep decision tree

models.append(BaggingClassifier(DecisionTreeClassifier(max_depth = 1, random_state=42), bootstrap=True, random_state=42))
models.append(BaggingClassifier(DecisionTreeClassifier(max_depth = 100, random_state=42), bootstrap=True, random_state=42))
# models.append(BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=100, max_samples=15, bootstrap=True, random_state=42))
# models.append(BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=100, max_samples=25, bootstrap=True, random_state=42))
# models.append(BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=100, max_samples=50, bootstrap=True, random_state=42))

bagging_train_accuracies = []
bagging_test_accuracies = []

for model in models:
    bagging_pipeline = Pipeline(steps = [
              ('bagging_classifier', model)
           ])
    
    
    print(" \n For model", model)
    bagging_model = bagging_pipeline.fit(X_train, y_train)

    print("\nTraining:")
    
    bagging_train_predictions = bagging_model.predict(X_train)
    bagging_train_accuracy = accuracy_score(y_train, bagging_train_predictions)
    bagging_train_accuracies.append(bagging_train_accuracy)
    print("training accuracy of model: ", bagging_train_accuracy)
    
    print("\n Testing: ")

    bagging_test_predictions = bagging_model.predict(X_test)
    bagging_test_accuracy = accuracy_score(y_test, bagging_test_predictions)
    bagging_test_accuracies.append(bagging_test_accuracy)
    print("testing accuracy of model:", bagging_test_accuracy)

    print("\n ================================================ \n")

accuracy_table = pd.DataFrame({'model' : models, 'Training Accuracy' : bagging_train_accuracies, 'Testing Accuracy' : bagging_test_accuracies})

ax = sns.lineplot(data=[accuracy_table['Training Accuracy'], accuracy_table['Testing Accuracy']], dashes=False, markers=True)
ax.set_xticks(range(len(accuracy_table['model'])))
ax.set_xticklabels(accuracy_table['model'])

"""As the maximum accuracy for adaboost model is 0.71, and for bagging model is 0.72; the bagging model with deep decision tree is efficient for classification of credit data."""



"""## Letter Dataset - Ada Boost

#### https://archive.ics.uci.edu/ml/datasets/letter+recognition
"""

import numpy as np
import pandas as pd

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score

# data
df = pd.read_csv('letter-recognition.data', header = None)

# # columns
# headerName = pd.read_csv('D:\IUB\COURSE WORK\FALL 2022\APPLIED MACHINE LEARNING\ASSIGNMENTS\ASSIGNMENT 4\spambase.names', sep = ':', skiprows=range(0, 33), header = None)
# col = list(headerName[0])
# col.append('Target')
# df.columns = col

headers = ["lettr","x-box", "y-box", "width", "high", "onpix", "x-bar", "y-bar", "x2bar", "y2bar", "xybar", "x2ybr", "xy2br", "x-ege", "xegvy", "y-ege", "yegvx"]

df.to_csv("lrec.csv", header=headers, index=False)

df2 = pd.read_csv("lrec.csv")

df2.head()

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

data_encode = df2

data_encode["lettr"]=label_encoder.fit_transform(data_encode["lettr"])
data_encode["lettr"].unique()

# df.head()

#splitting data
X_train, X_test, y_train, y_test = train_test_split(data_encode.drop(columns = "lettr").values, data_encode["lettr"].values, random_state = 42)

def epsilonT(wTi, y, predicted_y):

  et = wTi[(predicted_y != y)].sum()

  return et

def betaT(et):
  bt = 0.5 * (np.log((1 - et) / et))

  return bt

def updateWTi(wTi, y, predicted_y, bt):
  wt2 = (wTi * (np.exp(-bt * y * predicted_y)))

  return wt2

""" ### Shallow Tree"""

class model_ada:
    
    # constructor for model class
    def __init__(self):
        self.T = 0
        self.err_train = []
        self.err_pred = []
        self.bt_s = []
        self.ht_s = []
        
        

    def modelFit(self, X_train, y, T):
        
        self.T = T
        self.bt_s = [] 
        self.err_train = []
        
        w_i = np.ones(len(y)) * 1 / len(y) 

        for t in range(0, T):
            #TRAINING WITH WT
            # Using decision tree classifier
            ht = DecisionTreeClassifier(max_depth = 1)
            ht.fit(X_train, y, sample_weight = w_i)
            y_pred = ht.predict(X_train)
            
            self.ht_s.append(ht)

            # weighted training error calculation
            et = epsilonT(w_i, y, y_pred)
            self.err_train.append(et)

            # choose beta
            bt = betaT(et)
            self.bt_s.append(bt)

            #update all instance weights
            w_i = updateWTi(w_i, y, y_pred, bt)



    def modelPredict(self, X_test):
            # data with predictions[weak_data_predictions]
            weak = pd.DataFrame(index = range(len(X_test)), columns = range(self.T)) 

            # prediction
            for t in range(self.T):
                predict_yT = self.ht_s[t].predict(X_test) * self.bt_s[t]
                weak.iloc[:,t] = predict_yT

            y_pred = (1 * np.sign(weak.T.sum())).astype(int)

            return y_pred

# fitting
ab = model_ada()
ab.modelFit(X_train, y_train, T = 400)

# prediction
predictions = ab.modelPredict(X_test)

print("Classification report: \n", classification_report(y_test, predictions))

print("Score:", roc_auc_score(y_test, predictions))

"""### Deep Tree"""

class model_ada:
    
    # constructor for model class
    def __init__(self):
        self.T = 0
        self.err_train = []
        self.err_pred = []
        self.bt_s = []
        self.ht_s = []
        
        

    def modelFit(self, X_train, y, T):
        
        self.T = T
        self.bt_s = [] 
        self.err_train = []
        
        w_i = np.ones(len(y)) * 1 / len(y) 

        for t in range(0, T):
            #TRAINING WITH WT
            # Using decision tree classifier
            ht = DecisionTreeClassifier(max_depth = 100)
            ht.fit(X_train, y, sample_weight = w_i)
            y_pred = ht.predict(X_train)
            
            self.ht_s.append(ht)

            # weighted training error calculation
            et = epsilonT(w_i, y, y_pred)
            self.err_train.append(et)

            # choose beta
            bt = betaT(et)
            self.bt_s.append(bt)

            #update all instance weights
            w_i = updateWTi(w_i, y, y_pred, bt)



    def modelPredict(self, X_test):
            # data with predictions[weak_data_predictions]
            weak = pd.DataFrame(index = range(len(X_test)), columns = range(self.T)) 

            # prediction
            for t in range(self.T):
                predict_yT = self.ht_s[t].predict(X_test) * self.bt_s[t]
                weak.iloc[:,t] = predict_yT

            y_pred = (1 * np.sign(weak.T.sum())).astype(int)

            return y_pred

# fitting
ab = model_ada()
ab.modelFit(X_train, y_train, T = 400)

# prediction
predictions = ab.modelPredict(X_test)

print("Classification report: \n", classification_report(y_test, predictions))

print("Score:", roc_auc_score(y_test, predictions))





"""## Letter dataset - Bagging"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.pipeline import Pipeline

from sklearn.metrics import accuracy_score
import seaborn as sns

"""#### Training and testing bagging models for shallow and deep decision trees"""

Collection_bagging = ["model1", "model2"]

models = []

# creating 2 classifiers = first: shallow , second: deep decision tree

models.append(BaggingClassifier(DecisionTreeClassifier(max_depth = 1, random_state=42), bootstrap=True, random_state=42))
models.append(BaggingClassifier(DecisionTreeClassifier(max_depth = 100, random_state=42), bootstrap=True, random_state=42))
# models.append(BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=100, max_samples=15, bootstrap=True, random_state=42))
# models.append(BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=100, max_samples=25, bootstrap=True, random_state=42))
# models.append(BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=100, max_samples=50, bootstrap=True, random_state=42))

bagging_train_accuracies = []
bagging_test_accuracies = []

for model in models:
    bagging_pipeline = Pipeline(steps = [
              ('bagging_classifier', model)
           ])
    
    
    print(" \n For model", model)
    bagging_model = bagging_pipeline.fit(X_train, y_train)

    print("\nTraining:")
    
    bagging_train_predictions = bagging_model.predict(X_train)
    bagging_train_accuracy = accuracy_score(y_train, bagging_train_predictions)
    bagging_train_accuracies.append(bagging_train_accuracy)
    print("training accuracy of model: ", bagging_train_accuracy)
    
    print("\n Testing: ")

    bagging_test_predictions = bagging_model.predict(X_test)
    bagging_test_accuracy = accuracy_score(y_test, bagging_test_predictions)
    bagging_test_accuracies.append(bagging_test_accuracy)
    print("testing accuracy of model:", bagging_test_accuracy)

    print("\n ================================================ \n")

accuracy_table = pd.DataFrame({'model' : models, 'Training Accuracy' : bagging_train_accuracies, 'Testing Accuracy' : bagging_test_accuracies})

ax = sns.lineplot(data=[accuracy_table['Training Accuracy'], accuracy_table['Testing Accuracy']], dashes=False, markers=True)
ax.set_xticks(range(len(accuracy_table['model'])))
ax.set_xticklabels(accuracy_table['model'])

"""The maximum accuracy for bagging classifier for deep decision tree is 0.92. Thus, bagging model is better for letter dataset."""

